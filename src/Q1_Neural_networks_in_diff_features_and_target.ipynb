{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdeGjV6AaxWO",
        "outputId": "e0b2a139-31f4-4567-a932-0e164df0acfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Customer Aggregation Columns: ['customer_id', 'purchase_frequency', 'total_amount', 'recency', 'customer_lifetime', 'clv']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "customer_agg = pd.read_csv('/content/drive/MyDrive/Practicum/datasets after process/customer_agg.csv')\n",
        "\n",
        "# Display the columns of the dataset\n",
        "print(\"Customer Aggregation Columns:\", customer_agg.columns.tolist())\n",
        "\n",
        "# Define features and target\n",
        "features = ['customer_lifetime', 'recency', 'total_amount', 'clv', 'purchase_frequency']\n",
        "X = customer_agg[features]\n",
        "y = customer_agg['purchase_frequency']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the distribution of the target variable\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB3FqGkhd2IY",
        "outputId": "4480f706-7d74-4e4e-e507-fbfcee30c8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "purchase_frequency\n",
            "3     13910\n",
            "2     13382\n",
            "4     10271\n",
            "1      7762\n",
            "5      4313\n",
            "      ...  \n",
            "82        1\n",
            "79        1\n",
            "60        1\n",
            "84        1\n",
            "64        1\n",
            "Name: count, Length: 77, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exclude small classes from the dataset\n",
        "min_class_size = 2  # Minimum size for a class to be included\n",
        "\n",
        "# Filter out small classes\n",
        "valid_classes = y.value_counts()[y.value_counts() >= min_class_size].index\n",
        "X_valid = X[y.isin(valid_classes)]\n",
        "y_valid = y[y.isin(valid_classes)]\n",
        "\n",
        "# Perform stratified splitting on the filtered dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_valid, y_valid,\n",
        "    test_size=0.1,\n",
        "    stratify=y_valid,  # Ensure balanced classes in splits\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets to confirm the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Print the distribution of the target variable in the training and validation sets\n",
        "print(\"Training set target distribution:\", y_train.value_counts())\n",
        "print(\"Validation set target distribution:\", y_val.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjHtJCh2eQu6",
        "outputId": "c42a2500-083a-414d-81e1-9569e23fdb9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (54471, 5)\n",
            "Validation set shape: (6053, 5)\n",
            "Training set target distribution: purchase_frequency\n",
            "3     12519\n",
            "2     12044\n",
            "4      9244\n",
            "1      6986\n",
            "5      3882\n",
            "      ...  \n",
            "61        3\n",
            "76        3\n",
            "95        2\n",
            "71        2\n",
            "89        2\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Validation set target distribution: purchase_frequency\n",
            "3     1391\n",
            "2     1338\n",
            "4     1027\n",
            "1      776\n",
            "5      431\n",
            "6      130\n",
            "7       38\n",
            "27      31\n",
            "24      31\n",
            "29      31\n",
            "26      30\n",
            "22      30\n",
            "28      30\n",
            "25      29\n",
            "8       29\n",
            "23      28\n",
            "9       27\n",
            "30      27\n",
            "19      27\n",
            "17      26\n",
            "10      25\n",
            "18      25\n",
            "32      24\n",
            "21      24\n",
            "20      24\n",
            "14      24\n",
            "16      24\n",
            "15      24\n",
            "13      22\n",
            "31      21\n",
            "11      21\n",
            "12      20\n",
            "34      20\n",
            "33      19\n",
            "35      19\n",
            "36      18\n",
            "38      17\n",
            "41      16\n",
            "40      16\n",
            "37      16\n",
            "42      14\n",
            "39      13\n",
            "44      12\n",
            "43      12\n",
            "45      11\n",
            "46       9\n",
            "48       8\n",
            "47       8\n",
            "50       7\n",
            "49       6\n",
            "52       5\n",
            "53       4\n",
            "54       4\n",
            "51       4\n",
            "55       3\n",
            "57       2\n",
            "56       2\n",
            "58       1\n",
            "62       1\n",
            "59       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values for numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_train[features] = num_imputer.fit_transform(X_train[features])\n",
        "X_val[features] = num_imputer.transform(X_val[features])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[features] = scaler.fit_transform(X_train[features])\n",
        "X_val[features] = scaler.transform(X_val[features])"
      ],
      "metadata": {
        "id": "xDDqazUPe22D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojdnWJOFhEU-",
        "outputId": "9815e8bc-fed2-4489-efdf-ac3f0da26f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1703/1703 [==============================] - 7s 3ms/step - loss: 10.4990 - mae: 1.3500 - val_loss: 0.4830 - val_mae: 0.4077\n",
            "Epoch 2/10\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 2.0462 - mae: 0.6744 - val_loss: 0.9633 - val_mae: 0.4298\n",
            "Epoch 3/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 1.6608 - mae: 0.5652 - val_loss: 1.5914 - val_mae: 0.5376\n",
            "Epoch 4/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 1.2366 - mae: 0.4791 - val_loss: 2.6613 - val_mae: 0.7044\n",
            "Epoch 5/10\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 0.9905 - mae: 0.4207 - val_loss: 3.5750 - val_mae: 0.8405\n",
            "Epoch 6/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 0.8747 - mae: 0.3775 - val_loss: 2.9740 - val_mae: 0.7424\n",
            "Epoch 7/10\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 0.7876 - mae: 0.3443 - val_loss: 3.9161 - val_mae: 0.7852\n",
            "Epoch 8/10\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 0.7067 - mae: 0.3162 - val_loss: 5.3800 - val_mae: 0.9249\n",
            "Epoch 9/10\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 0.6293 - mae: 0.2917 - val_loss: 4.0396 - val_mae: 0.7663\n",
            "Epoch 10/10\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 0.5549 - mae: 0.2718 - val_loss: 3.9485 - val_mae: 0.7335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The current training output indicates that the model's validation loss and mean absolute error increase as training progresses. This suggests that the model might be overfitting, as it performs well on the training data but poorly on the validation data."
      ],
      "metadata": {
        "id": "QAMoM3T4jP5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686wbm7we5iT",
        "outputId": "3fcae909-8c5f-4a6c-b462-5be44c256212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1703/1703 [==============================] - 19s 9ms/step - loss: 7.3668 - mae: 1.1807 - val_loss: 0.1609 - val_mae: 0.2890\n",
            "Epoch 2/100\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 2.1955 - mae: 0.6849 - val_loss: 0.1236 - val_mae: 0.2393\n",
            "Epoch 3/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 1.9056 - mae: 0.6012 - val_loss: 0.0444 - val_mae: 0.1516\n",
            "Epoch 4/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 1.7122 - mae: 0.5483 - val_loss: 0.3790 - val_mae: 0.4220\n",
            "Epoch 5/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 1.4683 - mae: 0.4870 - val_loss: 0.3010 - val_mae: 0.4585\n",
            "Epoch 6/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 1.3177 - mae: 0.4416 - val_loss: 0.5368 - val_mae: 0.6784\n",
            "Epoch 7/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 0.9931 - mae: 0.3851 - val_loss: 1.0255 - val_mae: 0.8965\n",
            "Epoch 8/100\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 0.7884 - mae: 0.3425 - val_loss: 3.2333 - val_mae: 1.4078\n",
            "Epoch 9/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 0.5850 - mae: 0.2981 - val_loss: 4.0932 - val_mae: 1.5815\n",
            "Epoch 10/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 0.5019 - mae: 0.2820 - val_loss: 5.0892 - val_mae: 1.8608\n",
            "Epoch 11/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 0.4555 - mae: 0.2635 - val_loss: 5.0743 - val_mae: 1.9364\n",
            "Epoch 12/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 0.4131 - mae: 0.2520 - val_loss: 4.4463 - val_mae: 1.8495\n",
            "Epoch 13/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 0.3835 - mae: 0.2497 - val_loss: 4.6085 - val_mae: 1.8917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the updated neural network model with increased regularization and batch normalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define early stopping callback with increased patience\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Define a learning rate scheduler\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xGRLG7GjiME",
        "outputId": "a29f9049-932b-4a42-bf35-46bfd6c6defc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1703/1703 [==============================] - 16s 6ms/step - loss: 29.4057 - mae: 3.2382 - val_loss: 0.9752 - val_mae: 0.5126 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 14.6739 - mae: 2.3065 - val_loss: 0.9334 - val_mae: 0.6915 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 11.9630 - mae: 2.1338 - val_loss: 1.7221 - val_mae: 0.6695 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1703/1703 [==============================] - 9s 5ms/step - loss: 11.1714 - mae: 2.0981 - val_loss: 2.5110 - val_mae: 0.8704 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 10.3317 - mae: 2.0482 - val_loss: 1.4117 - val_mae: 0.7196 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 10.4960 - mae: 2.0299 - val_loss: 1.5707 - val_mae: 0.6582 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 10.5487 - mae: 2.0982 - val_loss: 2.6324 - val_mae: 0.8871 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 10.2241 - mae: 2.0648 - val_loss: 1.8372 - val_mae: 0.8643 - lr: 5.0000e-04\n",
            "Epoch 9/100\n",
            "1703/1703 [==============================] - 10s 6ms/step - loss: 10.1261 - mae: 2.0513 - val_loss: 2.2843 - val_mae: 0.9538 - lr: 5.0000e-04\n",
            "Epoch 10/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 9.8125 - mae: 2.0090 - val_loss: 1.9058 - val_mae: 0.8544 - lr: 5.0000e-04\n",
            "Epoch 11/100\n",
            "1703/1703 [==============================] - 8s 5ms/step - loss: 9.9420 - mae: 2.0263 - val_loss: 2.0237 - val_mae: 0.9282 - lr: 5.0000e-04\n",
            "Epoch 12/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 9.8367 - mae: 2.0337 - val_loss: 2.7800 - val_mae: 1.1720 - lr: 5.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Becasue of the features and target choose, the model is showing overfitting, so i'll start to use customer_lifetime, recency and total_amount as features and purchase_frequency as target."
      ],
      "metadata": {
        "id": "Ic0jc4CsqGyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['customer_lifetime', 'recency', 'total_amount']\n",
        "X = customer_agg[features]\n",
        "y = customer_agg['purchase_frequency']\n",
        "\n",
        "# Exclude small classes from the dataset\n",
        "min_class_size = 2  # Minimum size for a class to be included\n",
        "valid_classes = y.value_counts()[y.value_counts() >= min_class_size].index\n",
        "X_valid = X[y.isin(valid_classes)]\n",
        "y_valid = y[y.isin(valid_classes)]\n",
        "\n",
        "# Stratified splitting of the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_valid, y_valid,\n",
        "    test_size=0.1,\n",
        "    stratify=y_valid,  # Ensure balanced classes in splits\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets to confirm the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Print the distribution of the target variable in the training and validation sets\n",
        "print(\"Training set target distribution:\", y_train.value_counts())\n",
        "print(\"Validation set target distribution:\", y_val.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iULPxQkhqc1N",
        "outputId": "a4f34f9b-468c-446b-8669-65ac891861da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (54471, 3)\n",
            "Validation set shape: (6053, 3)\n",
            "Training set target distribution: purchase_frequency\n",
            "3     12519\n",
            "2     12044\n",
            "4      9244\n",
            "1      6986\n",
            "5      3882\n",
            "      ...  \n",
            "61        3\n",
            "76        3\n",
            "95        2\n",
            "71        2\n",
            "89        2\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Validation set target distribution: purchase_frequency\n",
            "3     1391\n",
            "2     1338\n",
            "4     1027\n",
            "1      776\n",
            "5      431\n",
            "6      130\n",
            "7       38\n",
            "27      31\n",
            "24      31\n",
            "29      31\n",
            "26      30\n",
            "22      30\n",
            "28      30\n",
            "25      29\n",
            "8       29\n",
            "23      28\n",
            "9       27\n",
            "30      27\n",
            "19      27\n",
            "17      26\n",
            "10      25\n",
            "18      25\n",
            "32      24\n",
            "21      24\n",
            "20      24\n",
            "14      24\n",
            "16      24\n",
            "15      24\n",
            "13      22\n",
            "31      21\n",
            "11      21\n",
            "12      20\n",
            "34      20\n",
            "33      19\n",
            "35      19\n",
            "36      18\n",
            "38      17\n",
            "41      16\n",
            "40      16\n",
            "37      16\n",
            "42      14\n",
            "39      13\n",
            "44      12\n",
            "43      12\n",
            "45      11\n",
            "46       9\n",
            "48       8\n",
            "47       8\n",
            "50       7\n",
            "49       6\n",
            "52       5\n",
            "53       4\n",
            "54       4\n",
            "51       4\n",
            "55       3\n",
            "57       2\n",
            "56       2\n",
            "58       1\n",
            "62       1\n",
            "59       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values for numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_train[features] = num_imputer.fit_transform(X_train[features])\n",
        "X_val[features] = num_imputer.transform(X_val[features])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[features] = scaler.fit_transform(X_train[features])\n",
        "X_val[features] = scaler.transform(X_val[features])"
      ],
      "metadata": {
        "id": "SX6zLDP1qo5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the simplified neural network model\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Define early stopping callback with increased patience\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Define a learning rate scheduler with a more aggressive reduction\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5zVTdaOqtha",
        "outputId": "084ecb53-c2f7-4f2d-9772-ab5d0597e4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1703/1703 [==============================] - 17s 8ms/step - loss: 53.3378 - mae: 3.9816 - val_loss: 32.8580 - val_mae: 3.1235 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 39.6764 - mae: 3.2971 - val_loss: 27.4331 - val_mae: 2.6748 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 35.7729 - mae: 2.9633 - val_loss: 25.4603 - val_mae: 2.5114 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 33.6600 - mae: 2.8107 - val_loss: 24.4538 - val_mae: 2.4610 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 32.8635 - mae: 2.7502 - val_loss: 25.3668 - val_mae: 2.4709 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 32.6105 - mae: 2.7112 - val_loss: 24.8846 - val_mae: 2.4411 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 32.4063 - mae: 2.7180 - val_loss: 24.0958 - val_mae: 2.5061 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 32.2881 - mae: 2.7157 - val_loss: 24.5576 - val_mae: 2.5073 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 31.7735 - mae: 2.7112 - val_loss: 24.6265 - val_mae: 2.4955 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 31.8619 - mae: 2.7286 - val_loss: 24.2849 - val_mae: 2.4444 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 31.0636 - mae: 2.7071 - val_loss: 24.3429 - val_mae: 2.4948 - lr: 5.0000e-04\n",
            "Epoch 12/100\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 30.8199 - mae: 2.7045 - val_loss: 24.3892 - val_mae: 2.4953 - lr: 5.0000e-04\n",
            "Epoch 13/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 31.0210 - mae: 2.7232 - val_loss: 24.4442 - val_mae: 2.5006 - lr: 5.0000e-04\n",
            "Epoch 14/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.8407 - mae: 2.7247 - val_loss: 23.9174 - val_mae: 2.4843 - lr: 2.5000e-04\n",
            "Epoch 15/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.5054 - mae: 2.7124 - val_loss: 24.0359 - val_mae: 2.4810 - lr: 2.5000e-04\n",
            "Epoch 16/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 30.7689 - mae: 2.7138 - val_loss: 24.2796 - val_mae: 2.4833 - lr: 2.5000e-04\n",
            "Epoch 17/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.9334 - mae: 2.7300 - val_loss: 24.2409 - val_mae: 2.5006 - lr: 2.5000e-04\n",
            "Epoch 18/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.5656 - mae: 2.7230 - val_loss: 24.0146 - val_mae: 2.4798 - lr: 1.2500e-04\n",
            "Epoch 19/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 30.8189 - mae: 2.7232 - val_loss: 23.8206 - val_mae: 2.4965 - lr: 1.2500e-04\n",
            "Epoch 20/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.7294 - mae: 2.7316 - val_loss: 24.0699 - val_mae: 2.4871 - lr: 1.2500e-04\n",
            "Epoch 21/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.7566 - mae: 2.7256 - val_loss: 24.0518 - val_mae: 2.4823 - lr: 1.2500e-04\n",
            "Epoch 22/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 30.4727 - mae: 2.7205 - val_loss: 24.0534 - val_mae: 2.4849 - lr: 1.2500e-04\n",
            "Epoch 23/100\n",
            "1703/1703 [==============================] - 7s 4ms/step - loss: 30.5843 - mae: 2.7332 - val_loss: 23.8383 - val_mae: 2.4808 - lr: 6.2500e-05\n",
            "Epoch 24/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.5789 - mae: 2.7258 - val_loss: 24.1459 - val_mae: 2.4832 - lr: 6.2500e-05\n",
            "Epoch 25/100\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 30.8227 - mae: 2.7404 - val_loss: 24.2020 - val_mae: 2.4896 - lr: 6.2500e-05\n",
            "Epoch 26/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.7937 - mae: 2.7314 - val_loss: 24.0692 - val_mae: 2.4898 - lr: 3.1250e-05\n",
            "Epoch 27/100\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 30.9156 - mae: 2.7397 - val_loss: 24.1946 - val_mae: 2.4917 - lr: 3.1250e-05\n",
            "Epoch 28/100\n",
            "1703/1703 [==============================] - 6s 3ms/step - loss: 30.4516 - mae: 2.7292 - val_loss: 24.0458 - val_mae: 2.4837 - lr: 3.1250e-05\n",
            "Epoch 29/100\n",
            "1703/1703 [==============================] - 6s 4ms/step - loss: 31.0707 - mae: 2.7496 - val_loss: 24.0521 - val_mae: 2.4862 - lr: 1.5625e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the simplified neural network model\n",
        "model = Sequential([\n",
        "    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiUM9931xT1b",
        "outputId": "1e5b3fa9-2aaf-4a40-dbe6-3af8f2c7b9b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1703/1703 [==============================] - 7s 3ms/step - loss: 53.9056 - mae: 4.0076 - val_loss: 32.2711 - val_mae: 3.2029\n",
            "Epoch 2/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 38.8997 - mae: 3.2848 - val_loss: 27.3438 - val_mae: 2.7154\n",
            "Epoch 3/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 35.5175 - mae: 2.9764 - val_loss: 25.4280 - val_mae: 2.5230\n",
            "Epoch 4/10\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 34.4358 - mae: 2.8462 - val_loss: 24.4798 - val_mae: 2.4745\n",
            "Epoch 5/10\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 33.5029 - mae: 2.7847 - val_loss: 24.3346 - val_mae: 2.4545\n",
            "Epoch 6/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 32.6822 - mae: 2.7450 - val_loss: 23.9611 - val_mae: 2.4564\n",
            "Epoch 7/10\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 31.9171 - mae: 2.7258 - val_loss: 24.1797 - val_mae: 2.4307\n",
            "Epoch 8/10\n",
            "1703/1703 [==============================] - 5s 3ms/step - loss: 31.7897 - mae: 2.7366 - val_loss: 24.5834 - val_mae: 2.4902\n",
            "Epoch 9/10\n",
            "1703/1703 [==============================] - 4s 2ms/step - loss: 31.6188 - mae: 2.7347 - val_loss: 23.8560 - val_mae: 2.4353\n",
            "Epoch 10/10\n",
            "1703/1703 [==============================] - 4s 3ms/step - loss: 31.1935 - mae: 2.7229 - val_loss: 25.1566 - val_mae: 2.4871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Convert predictions and true values to bins\n",
        "bins = [-1, 10, 20, float('inf')]\n",
        "labels = ['low', 'medium', 'high']\n",
        "y_pred_binned = pd.cut(y_pred.flatten(), bins=bins, labels=labels)\n",
        "y_val_binned = pd.cut(y_val, bins=bins, labels=labels)\n",
        "\n",
        "# Evaluate the model's performance using classification metrics\n",
        "accuracy = accuracy_score(y_val_binned, y_pred_binned)\n",
        "conf_matrix = confusion_matrix(y_val_binned, y_pred_binned)\n",
        "class_report = classification_report(y_val_binned, y_pred_binned, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zreIHFFGzs_",
        "outputId": "1dbd4710-d86b-4c74-e31f-2be5ba4721f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190/190 [==============================] - 1s 3ms/step - loss: 25.1566 - mae: 2.4871\n",
            "Validation Loss: 25.1566\n",
            "Validation MAE: 2.4871\n",
            "190/190 [==============================] - 1s 4ms/step\n",
            "Accuracy: 0.9181\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 439   52  113]\n",
            " [  40 5052  120]\n",
            " [  67  104   66]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.80      0.73      0.76       604\n",
            "         low       0.97      0.97      0.97      5212\n",
            "      medium       0.22      0.28      0.25       237\n",
            "\n",
            "    accuracy                           0.92      6053\n",
            "   macro avg       0.66      0.66      0.66      6053\n",
            "weighted avg       0.92      0.92      0.92      6053\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "customer_agg = pd.read_csv('/content/drive/MyDrive/Practicum/datasets after process/customer_agg.csv')\n",
        "\n",
        "# Display the columns of the dataset\n",
        "print(\"Customer Aggregation Columns:\", customer_agg.columns.tolist())\n",
        "\n",
        "# Define features and target\n",
        "features = ['customer_lifetime', 'recency', 'total_amount']\n",
        "X = customer_agg[features]\n",
        "y = customer_agg['purchase_frequency']\n",
        "\n",
        "# Exclude small classes from the dataset\n",
        "min_class_size = 2  # Minimum size for a class to be included\n",
        "valid_classes = y.value_counts()[y.value_counts() >= min_class_size].index\n",
        "X_valid = X[y.isin(valid_classes)]\n",
        "y_valid = y[y.isin(valid_classes)]\n",
        "\n",
        "# Stratified splitting of the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_valid, y_valid,\n",
        "    test_size=0.1,\n",
        "    stratify=y_valid,  # Ensure balanced classes in splits\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets to confirm the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Print the distribution of the target variable in the training and validation sets\n",
        "print(\"Training set target distribution:\", y_train.value_counts())\n",
        "print(\"Validation set target distribution:\", y_val.value_counts())\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_train[features] = num_imputer.fit_transform(X_train[features])\n",
        "X_val[features] = num_imputer.transform(X_val[features])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[features] = scaler.fit_transform(X_train[features])\n",
        "X_val[features] = scaler.transform(X_val[features])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuwtumR9XJW3",
        "outputId": "bfa114a3-a33d-4218-900f-5c07d4205183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Customer Aggregation Columns: ['customer_id', 'purchase_frequency', 'total_amount', 'recency', 'customer_lifetime', 'clv']\n",
            "Training set shape: (54471, 3)\n",
            "Validation set shape: (6053, 3)\n",
            "Training set target distribution: purchase_frequency\n",
            "3     12519\n",
            "2     12044\n",
            "4      9244\n",
            "1      6986\n",
            "5      3882\n",
            "      ...  \n",
            "61        3\n",
            "76        3\n",
            "95        2\n",
            "71        2\n",
            "89        2\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Validation set target distribution: purchase_frequency\n",
            "3     1391\n",
            "2     1338\n",
            "4     1027\n",
            "1      776\n",
            "5      431\n",
            "6      130\n",
            "7       38\n",
            "27      31\n",
            "24      31\n",
            "29      31\n",
            "26      30\n",
            "22      30\n",
            "28      30\n",
            "25      29\n",
            "8       29\n",
            "23      28\n",
            "9       27\n",
            "30      27\n",
            "19      27\n",
            "17      26\n",
            "10      25\n",
            "18      25\n",
            "32      24\n",
            "21      24\n",
            "20      24\n",
            "14      24\n",
            "16      24\n",
            "15      24\n",
            "13      22\n",
            "31      21\n",
            "11      21\n",
            "12      20\n",
            "34      20\n",
            "33      19\n",
            "35      19\n",
            "36      18\n",
            "38      17\n",
            "41      16\n",
            "40      16\n",
            "37      16\n",
            "42      14\n",
            "39      13\n",
            "44      12\n",
            "43      12\n",
            "45      11\n",
            "46       9\n",
            "48       8\n",
            "47       8\n",
            "50       7\n",
            "49       6\n",
            "52       5\n",
            "53       4\n",
            "54       4\n",
            "51       4\n",
            "55       3\n",
            "57       2\n",
            "56       2\n",
            "58       1\n",
            "62       1\n",
            "59       1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# Apply SMOTE to the training data with adjusted k_neighbors parameter\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the new distribution of the target variable after applying SMOTE\n",
        "print(\"Training set target distribution after SMOTE:\", y_train_smote.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyR1QJ6qYgG3",
        "outputId": "a33f02c4-e2cd-4844-ad63-a43364554d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set target distribution after SMOTE: purchase_frequency\n",
            "3     12519\n",
            "58    12519\n",
            "40    12519\n",
            "62    12519\n",
            "11    12519\n",
            "      ...  \n",
            "15    12519\n",
            "36    12519\n",
            "34    12519\n",
            "19    12519\n",
            "89    12519\n",
            "Name: count, Length: 66, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simplified neural network model with adjusted hyperparameters\n",
        "model = Sequential([\n",
        "    Dense(16, activation='relu', input_shape=(X_train_smote.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(8, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model for 10 epochs\n",
        "history = model.fit(\n",
        "    X_train_smote, y_train_smote,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oukJOEXpZOT9",
        "outputId": "8532ee58-f9ca-4335-b18a-32ae669cff6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "12911/12911 [==============================] - 46s 3ms/step - loss: 280.9346 - mae: 12.4810 - val_loss: 53.9442 - val_mae: 5.3832\n",
            "Epoch 2/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 210.3510 - mae: 11.0843 - val_loss: 61.8111 - val_mae: 6.4617\n",
            "Epoch 3/10\n",
            "12911/12911 [==============================] - 35s 3ms/step - loss: 190.2017 - mae: 10.6177 - val_loss: 67.1852 - val_mae: 7.0014\n",
            "Epoch 4/10\n",
            "12911/12911 [==============================] - 36s 3ms/step - loss: 182.1875 - mae: 10.4725 - val_loss: 75.4944 - val_mae: 7.5995\n",
            "Epoch 5/10\n",
            "12911/12911 [==============================] - 35s 3ms/step - loss: 179.3069 - mae: 10.4357 - val_loss: 82.2790 - val_mae: 7.9720\n",
            "Epoch 6/10\n",
            "12911/12911 [==============================] - 35s 3ms/step - loss: 179.1454 - mae: 10.4468 - val_loss: 85.2534 - val_mae: 8.2311\n",
            "Epoch 7/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 178.7246 - mae: 10.4479 - val_loss: 85.8258 - val_mae: 8.2925\n",
            "Epoch 8/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 178.3884 - mae: 10.4349 - val_loss: 85.4740 - val_mae: 8.2511\n",
            "Epoch 9/10\n",
            "12911/12911 [==============================] - 40s 3ms/step - loss: 178.3788 - mae: 10.4351 - val_loss: 89.8156 - val_mae: 8.4806\n",
            "Epoch 10/10\n",
            "12911/12911 [==============================] - 36s 3ms/step - loss: 177.8688 - mae: 10.4187 - val_loss: 87.5049 - val_mae: 8.3415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Load the dataset\n",
        "customer_agg = pd.read_csv('/content/drive/MyDrive/Practicum/datasets after process/customer_agg.csv')\n",
        "\n",
        "# Display the columns of the dataset\n",
        "print(\"Customer Aggregation Columns:\", customer_agg.columns.tolist())\n",
        "\n",
        "# Define features and target\n",
        "features = ['customer_lifetime', 'recency', 'total_amount']\n",
        "X = customer_agg[features]\n",
        "y = customer_agg['purchase_frequency']\n",
        "\n",
        "# Exclude small classes from the dataset\n",
        "min_class_size = 2  # Minimum size for a class to be included\n",
        "valid_classes = y.value_counts()[y.value_counts() >= min_class_size].index\n",
        "X_valid = X[y.isin(valid_classes)]\n",
        "y_valid = y[y.isin(valid_classes)]\n",
        "\n",
        "# Stratified splitting of the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_valid, y_valid,\n",
        "    test_size=0.1,\n",
        "    stratify=y_valid,  # Ensure balanced classes in splits\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets to confirm the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Print the distribution of the target variable in the training and validation sets\n",
        "print(\"Training set target distribution:\", y_train.value_counts())\n",
        "print(\"Validation set target distribution:\", y_val.value_counts())\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_train[features] = num_imputer.fit_transform(X_train[features])\n",
        "X_val[features] = num_imputer.transform(X_val[features])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[features] = scaler.fit_transform(X_train[features])\n",
        "X_val[features] = scaler.transform(X_val[features])\n",
        "\n",
        "# Apply SMOTE to the training data with adjusted k_neighbors parameter\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the new distribution of the target variable after applying SMOTE\n",
        "print(\"Training set target distribution after SMOTE:\", y_train_smote.value_counts())\n",
        "\n",
        "# Define a more simplified neural network model with increased regularization\n",
        "model = Sequential([\n",
        "    Dense(8, activation='relu', input_shape=(X_train_smote.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(4, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model for 10 epochs with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    X_train_smote, y_train_smote,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKUdRhhLpihT",
        "outputId": "19895dcc-86a6-4a1e-fa54-35f510454c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Customer Aggregation Columns: ['customer_id', 'purchase_frequency', 'total_amount', 'recency', 'customer_lifetime', 'clv']\n",
            "Training set shape: (54471, 3)\n",
            "Validation set shape: (6053, 3)\n",
            "Training set target distribution: purchase_frequency\n",
            "3     12519\n",
            "2     12044\n",
            "4      9244\n",
            "1      6986\n",
            "5      3882\n",
            "      ...  \n",
            "61        3\n",
            "76        3\n",
            "95        2\n",
            "71        2\n",
            "89        2\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Validation set target distribution: purchase_frequency\n",
            "3     1391\n",
            "2     1338\n",
            "4     1027\n",
            "1      776\n",
            "5      431\n",
            "6      130\n",
            "7       38\n",
            "27      31\n",
            "24      31\n",
            "29      31\n",
            "26      30\n",
            "22      30\n",
            "28      30\n",
            "25      29\n",
            "8       29\n",
            "23      28\n",
            "9       27\n",
            "30      27\n",
            "19      27\n",
            "17      26\n",
            "10      25\n",
            "18      25\n",
            "32      24\n",
            "21      24\n",
            "20      24\n",
            "14      24\n",
            "16      24\n",
            "15      24\n",
            "13      22\n",
            "31      21\n",
            "11      21\n",
            "12      20\n",
            "34      20\n",
            "33      19\n",
            "35      19\n",
            "36      18\n",
            "38      17\n",
            "41      16\n",
            "40      16\n",
            "37      16\n",
            "42      14\n",
            "39      13\n",
            "44      12\n",
            "43      12\n",
            "45      11\n",
            "46       9\n",
            "48       8\n",
            "47       8\n",
            "50       7\n",
            "49       6\n",
            "52       5\n",
            "53       4\n",
            "54       4\n",
            "51       4\n",
            "55       3\n",
            "57       2\n",
            "56       2\n",
            "58       1\n",
            "62       1\n",
            "59       1\n",
            "Name: count, dtype: int64\n",
            "Training set target distribution after SMOTE: purchase_frequency\n",
            "3     12519\n",
            "58    12519\n",
            "40    12519\n",
            "62    12519\n",
            "11    12519\n",
            "      ...  \n",
            "15    12519\n",
            "36    12519\n",
            "34    12519\n",
            "19    12519\n",
            "89    12519\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Epoch 1/10\n",
            "12911/12911 [==============================] - 39s 3ms/step - loss: 500.0265 - mae: 16.5609 - val_loss: 40.9239 - val_mae: 4.4028\n",
            "Epoch 2/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 305.5939 - mae: 13.0178 - val_loss: 76.8567 - val_mae: 7.9676\n",
            "Epoch 3/10\n",
            "12911/12911 [==============================] - 37s 3ms/step - loss: 269.8038 - mae: 12.4873 - val_loss: 146.8330 - val_mae: 11.5321\n",
            "Epoch 4/10\n",
            "12911/12911 [==============================] - 37s 3ms/step - loss: 254.6351 - mae: 12.3641 - val_loss: 216.7982 - val_mae: 14.1176\n",
            "Epoch 5/10\n",
            "12911/12911 [==============================] - 36s 3ms/step - loss: 248.9991 - mae: 12.3794 - val_loss: 257.1803 - val_mae: 15.4016\n",
            "Epoch 6/10\n",
            "12911/12911 [==============================] - 36s 3ms/step - loss: 247.6756 - mae: 12.4136 - val_loss: 276.2719 - val_mae: 15.9747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Convert predictions and true values to bins\n",
        "bins = [-1, 10, 20, float('inf')]\n",
        "labels = ['low', 'medium', 'high']\n",
        "y_pred_binned = pd.cut(y_pred.flatten(), bins=bins, labels=labels)\n",
        "y_val_binned = pd.cut(y_val, bins=bins, labels=labels)\n",
        "\n",
        "# Evaluate the model's performance using classification metrics\n",
        "accuracy = accuracy_score(y_val_binned, y_pred_binned)\n",
        "conf_matrix = confusion_matrix(y_val_binned, y_pred_binned)\n",
        "class_report = classification_report(y_val_binned, y_pred_binned, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2J53QsesQ9E",
        "outputId": "c5cad495-0da1-4c77-c99b-80f3a364cf7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "190/190 [==============================] - 0s 2ms/step - loss: 40.9239 - mae: 4.4028\n",
            "Validation Loss: 40.9239\n",
            "Validation MAE: 4.4028\n",
            "190/190 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8621\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 545    4   55]\n",
            " [ 151 4588  473]\n",
            " [ 124   28   85]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.66      0.90      0.77       604\n",
            "         low       0.99      0.88      0.93      5212\n",
            "      medium       0.14      0.36      0.20       237\n",
            "\n",
            "    accuracy                           0.86      6053\n",
            "   macro avg       0.60      0.71      0.63      6053\n",
            "weighted avg       0.93      0.86      0.89      6053\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Class Imbalance**:\n",
        "- The 'medium' class remains challenging. This is a common issue in imbalanced datasets. Despite applying SMOTE, the inherent difficulty of the medium class is evident.\n",
        "\n",
        "2. **Model Performance**:\n",
        "- The model shows improved performance in the 'high' and 'low' classes.\n",
        "- The 'medium' class continues to have low precision and recall, indicating it is still underrepresented and challenging to predict accurately."
      ],
      "metadata": {
        "id": "smPwY14YuT0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features and target\n",
        "features = ['customer_lifetime', 'recency', 'total_amount']\n",
        "X = customer_agg[features]\n",
        "y = customer_agg['purchase_frequency']\n",
        "\n",
        "# Exclude small classes from the dataset\n",
        "min_class_size = 2  # Minimum size for a class to be included\n",
        "valid_classes = y.value_counts()[y.value_counts() >= min_class_size].index\n",
        "X_valid = X[y.isin(valid_classes)]\n",
        "y_valid = y[y.isin(valid_classes)]\n",
        "\n",
        "# Stratified splitting of the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        " X_valid, y_valid,\n",
        " test_size=0.1,\n",
        " stratify=y_valid,  # Ensure balanced classes in splits\n",
        " random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the datasets to confirm the split\n",
        "print(\"Training set shape:\", X_train.shape)\n",
        "print(\"Validation set shape:\", X_val.shape)\n",
        "\n",
        "# Print the distribution of the target variable in the training and validation sets\n",
        "print(\"Training set target distribution:\", y_train.value_counts())\n",
        "print(\"Validation set target distribution:\", y_val.value_counts())\n",
        "\n",
        "# Impute missing values for numerical features\n",
        "num_imputer = SimpleImputer(strategy='mean')\n",
        "X_train[features] = num_imputer.fit_transform(X_train[features])\n",
        "X_val[features] = num_imputer.transform(X_val[features])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train[features] = scaler.fit_transform(X_train[features])\n",
        "X_val[features] = scaler.transform(X_val[features])\n",
        "\n",
        "# Apply SMOTE to the training data with adjusted k_neighbors parameter\n",
        "smote = SMOTE(random_state=42, k_neighbors=1)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Check the new distribution of the target variable after applying SMOTE\n",
        "print(\"Training set target distribution after SMOTE:\", y_train_smote.value_counts())\n",
        "\n",
        "# Define a more simplified neural network model with increased regularization\n",
        "model = Sequential([\n",
        " Dense(8, activation='relu', input_shape=(X_train_smote.shape[1],), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        " Dropout(0.3),\n",
        " Dense(4, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        " Dropout(0.3),\n",
        " Dense(1, activation='linear')  # Assuming a regression task for purchase_frequency\n",
        "])\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model for 10 epochs with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "history = model.fit(\n",
        " X_train_smote, y_train_smote,\n",
        " validation_data=(X_val, y_val),\n",
        " epochs=10,\n",
        " batch_size=64,\n",
        " callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_mae = model.evaluate(X_val, y_val)\n",
        "print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Convert predictions and true values to bins\n",
        "bins = [-1, 10, 20, float('inf')]\n",
        "labels = ['low', 'medium', 'high']\n",
        "y_pred_binned = pd.cut(y_pred.flatten(), bins=bins, labels=labels)\n",
        "y_val_binned = pd.cut(y_val, bins=bins, labels=labels)\n",
        "\n",
        "# Evaluate the model's performance using classification metrics\n",
        "accuracy = accuracy_score(y_val_binned, y_pred_binned)\n",
        "conf_matrix = confusion_matrix(y_val_binned, y_pred_binned)\n",
        "class_report = classification_report(y_val_binned, y_pred_binned, zero_division=0)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZAt5YwxjIw",
        "outputId": "68542f84-67c7-42b7-a54e-aee18089a0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (54471, 3)\n",
            "Validation set shape: (6053, 3)\n",
            "Training set target distribution: purchase_frequency\n",
            "3     12519\n",
            "2     12044\n",
            "4      9244\n",
            "1      6986\n",
            "5      3882\n",
            "      ...  \n",
            "61        3\n",
            "76        3\n",
            "95        2\n",
            "71        2\n",
            "89        2\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Validation set target distribution: purchase_frequency\n",
            "3     1391\n",
            "2     1338\n",
            "4     1027\n",
            "1      776\n",
            "5      431\n",
            "6      130\n",
            "7       38\n",
            "27      31\n",
            "24      31\n",
            "29      31\n",
            "26      30\n",
            "22      30\n",
            "28      30\n",
            "25      29\n",
            "8       29\n",
            "23      28\n",
            "9       27\n",
            "30      27\n",
            "19      27\n",
            "17      26\n",
            "10      25\n",
            "18      25\n",
            "32      24\n",
            "21      24\n",
            "20      24\n",
            "14      24\n",
            "16      24\n",
            "15      24\n",
            "13      22\n",
            "31      21\n",
            "11      21\n",
            "12      20\n",
            "34      20\n",
            "33      19\n",
            "35      19\n",
            "36      18\n",
            "38      17\n",
            "41      16\n",
            "40      16\n",
            "37      16\n",
            "42      14\n",
            "39      13\n",
            "44      12\n",
            "43      12\n",
            "45      11\n",
            "46       9\n",
            "48       8\n",
            "47       8\n",
            "50       7\n",
            "49       6\n",
            "52       5\n",
            "53       4\n",
            "54       4\n",
            "51       4\n",
            "55       3\n",
            "57       2\n",
            "56       2\n",
            "58       1\n",
            "62       1\n",
            "59       1\n",
            "Name: count, dtype: int64\n",
            "Training set target distribution after SMOTE: purchase_frequency\n",
            "3     12519\n",
            "58    12519\n",
            "40    12519\n",
            "62    12519\n",
            "11    12519\n",
            "      ...  \n",
            "15    12519\n",
            "36    12519\n",
            "34    12519\n",
            "19    12519\n",
            "89    12519\n",
            "Name: count, Length: 66, dtype: int64\n",
            "Epoch 1/10\n",
            "12911/12911 [==============================] - 50s 3ms/step - loss: 749.9483 - mae: 20.8146 - val_loss: 59.6824 - val_mae: 5.0337\n",
            "Epoch 2/10\n",
            "12911/12911 [==============================] - 41s 3ms/step - loss: 511.2240 - mae: 16.9554 - val_loss: 57.0917 - val_mae: 4.9229\n",
            "Epoch 3/10\n",
            "12911/12911 [==============================] - 36s 3ms/step - loss: 483.4154 - mae: 16.4102 - val_loss: 52.7571 - val_mae: 4.7377\n",
            "Epoch 4/10\n",
            "12911/12911 [==============================] - 35s 3ms/step - loss: 455.2013 - mae: 15.8680 - val_loss: 47.5829 - val_mae: 4.3657\n",
            "Epoch 5/10\n",
            "12911/12911 [==============================] - 40s 3ms/step - loss: 427.9345 - mae: 15.3147 - val_loss: 45.0241 - val_mae: 4.5508\n",
            "Epoch 6/10\n",
            "12911/12911 [==============================] - 39s 3ms/step - loss: 400.5485 - mae: 14.7922 - val_loss: 45.1991 - val_mae: 4.9690\n",
            "Epoch 7/10\n",
            "12911/12911 [==============================] - 40s 3ms/step - loss: 377.7941 - mae: 14.3537 - val_loss: 47.6830 - val_mae: 5.4378\n",
            "Epoch 8/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 357.8907 - mae: 13.9570 - val_loss: 52.6951 - val_mae: 5.9461\n",
            "Epoch 9/10\n",
            "12911/12911 [==============================] - 38s 3ms/step - loss: 338.6773 - mae: 13.6046 - val_loss: 57.2089 - val_mae: 6.3914\n",
            "Epoch 10/10\n",
            "12911/12911 [==============================] - 35s 3ms/step - loss: 326.6676 - mae: 13.3991 - val_loss: 62.8630 - val_mae: 6.8513\n",
            "190/190 [==============================] - 0s 2ms/step - loss: 45.0241 - mae: 4.5508\n",
            "Validation Loss: 45.0241\n",
            "Validation MAE: 4.5508\n",
            "190/190 [==============================] - 0s 2ms/step\n",
            "Accuracy: 0.8348\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 555    2   47]\n",
            " [ 198 4424  590]\n",
            " [ 142   21   74]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.62      0.92      0.74       604\n",
            "         low       0.99      0.85      0.92      5212\n",
            "      medium       0.10      0.31      0.16       237\n",
            "\n",
            "    accuracy                           0.83      6053\n",
            "   macro avg       0.57      0.69      0.60      6053\n",
            "weighted avg       0.92      0.83      0.87      6053\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "### Research Question 1: What are the key predictive indicators of a customer's purchasing behavior?\n",
        "\n",
        "To address this research question, a Neural Network model was trained on customer data to predict the `purchase_frequency` using the following features:\n",
        "- `customer_lifetime`\n",
        "- `recency`\n",
        "- `total_amount`\n",
        "\n",
        "### Model Performance Summary\n",
        "\n",
        "The model achieved an overall accuracy of 83.48% on the validation set. Below is a detailed analysis of the model's performance:\n",
        "\n",
        "- **Accuracy**: 0.8348\n",
        "- **Validation Loss**: 45.0241\n",
        "- **Validation MAE**: 4.5508\n",
        "\n",
        "### Key Predictive Indicators\n",
        "\n",
        "Based on the model's performance and the features used, the key predictive indicators of a customer's purchasing behavior are as follows:\n",
        "\n",
        "1. **Customer Lifetime**:\n",
        "   - This feature indicates the total duration a customer has been active. It is a significant predictor as it provides insights into the customer's longevity with the business.\n",
        "\n",
        "2. **Recency**:\n",
        "   - This feature measures the time since the last purchase. It is crucial as it helps in understanding how recently a customer made a purchase, indicating their engagement level.\n",
        "\n",
        "3. **Total Amount**:\n",
        "   - The total amount spent by the customer is a direct indicator of their purchasing behavior. It reflects the customer's spending capacity and frequency of purchases.\n",
        "\n",
        "### Analysis\n",
        "\n",
        "- **High and Low Classes**:\n",
        "  - The model performs well in predicting the 'low' class with a precision of 0.99 and a recall of 0.85, resulting in a high f1-score of 0.92.\n",
        "  - The 'high' class has moderate performance with a precision of 0.62 and a recall of 0.92, indicating that while it is relatively good at predicting 'high' instances, it has a higher tendency to classify some 'low' instances as 'high'.\n",
        "\n",
        "- **Medium Class**:\n",
        "  - The 'medium' class prediction is still poor with a precision of 0.10 and a recall of 0.31. This indicates that the model struggles to accurately predict this class, resulting in a very low f1-score of 0.16."
      ],
      "metadata": {
        "id": "sI_vP0EEpiah"
      }
    }
  ]
}